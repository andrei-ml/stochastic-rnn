## Introduction

In this folder we develop a pipeline to emulate the dynamics of atmosperic regimes using SRNN.

- We use geopotential height (HGT) daily winter data generated by MPI-ESM-HR model in picontrol CMIP6 experiment
- We employ Kernel PCA to characterize the target atmospheric regimes, and aim to emulate them with SRNN
- We construct SRNN using **jointly PCs and kernel PCs** and assess its performance

The python scripts are splitted into steps which should be run sequentially. Their naming format is **step#***.

To run them, additional packages are needed:

```bash
pip install scikit-learn xarray joblib
```

## Step 1: Data prepapration and preprocessing

Here we assume that the following data was downloaded in a dedicated folder (see the script contetns to modify the folder name):

 - The data to download is a daily geopotential height (zg) of CMIP6 piControl experiment of the model MPI-ESM-HR, version 20190710

 - Its .nc files are downloaded from https://esgf-metagrid.cloud.dkrz.de/ into the folder 'data_path' specified in the 'prepare_data.py'

 - As of the date for this notebook creation, the search parameter to match is: 'CMIP6.CMIP.MPI-M.MPI-ESM1-2-HR.piControl.r1i1p1f1.day.zg.gn'

Data preparation pipeline implemented (look the script for details):

 - Extracting the years 2276–2349, from the first April 1 to the last March 31 (73 years in total)
 - Selecting values at 100 hPa level and for latitudes north of 50° N
 - Removing March 31 from each leap year
 - Linear detrending
 - Subtracting a smoothed (15-day Gaussian) climatology
 - Training data: selecting batches of September–March intervals (212 days $\times$ 73) 
 - Evaluation data: repeat the above for the years 2198–2271

Preprocessing:

- Geographical weighting
- Computing and normalizing 3 PCA components for training data
- Computing and normalizing 3 Kernel PCA components for training data, which can be interpreted in terms of atmospheric regimes
- Projecting evaluation data to the same PCA/Kernel PCA space

## Step 2: SRNN training

We train SRNN in two configurations (marked a and b). The main one (a) uses PCs as inputs and jointly PCs and Kernel PCS as outputs, and thus can predict the PC time series together with the corresponding Kernel PCs, doing the prediction and, implicitly, Kernel PCA mapping without the requirement to predict the full data field. The configuration "b" uses only Kernel PCs time series as inputs and outputs, like in a classic RNN for time series prediction. It is shown to perform worse as an emulator of Kernel PC dynamics.

We use:
- 5-fold cross-validation
- Neagtive logarithm of Bayesian posterior PDF as training loss (equivalent to negative log-likelihood with L2-regularization)
- Negative log-likelihood as validation loss
- BFGS minimization algorithm with early stopping based on the absence of validation loss improvement. Note that BFGS minimization is typically efficient for the low-dimensional time series, while in a high-dimensional case SGD minimizers could be used.
- We optimize SRNN lag and number of neurons by simple lazy grid search.

## Step 3: Selection of optimal model

## Step 4: Making predictions

## Step 5: Plotting the results

Steps 3-5 are implemented as jupyter notebooks. See the comments there.

